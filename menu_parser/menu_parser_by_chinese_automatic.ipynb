{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "from statistics import mean \n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "# Custom module imports (assuming they are necessary for your project)\n",
    "sys.path.append(\"../../\")\n",
    "from models.bounding_box import FeatureType, Point, BoundingBox, DSU\n",
    "from models.dish_segmenter import Dish\n",
    "from models.word_unit import WordUnit\n",
    "from utils.cv_preprocess import *\n",
    "from utils.file_utils import *\n",
    "from utils.nlp_preprocess import *\n",
    "\n",
    "# IPython specific import for inline display\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_menu(filein):\n",
    "    base_name = os.path.basename(filein)\n",
    "    file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "    raw_ocr_directory = '../../dataset/menu_photo_ocr_raw/'\n",
    "    raw_ocr_filename = file_name_without_extension + \"_raw_annotation.json\"\n",
    "    raw_ocr_path = os.path.join(raw_ocr_directory, raw_ocr_filename)\n",
    "\n",
    "    document = load_json(raw_ocr_path)\n",
    "    image = Image.open(filein)\n",
    "    \n",
    "    \n",
    "\n",
    "    bounds = process_bounds_in_words(document)\n",
    "    \n",
    "\n",
    "    price_bounds = extract_price_bounds(bounds)\n",
    "    filtered_bounds, chinese_bbox, english_bbox = filter_and_classify_bounds(bounds)\n",
    "\n",
    "    # locator_bounds = {\"price_bounds\": price_bounds}\n",
    "    locator_bounds = {\"chinese_bbox\": chinese_bbox}\n",
    "\n",
    "    overlap_threshold_list = [0.3, 0.4, 0.5]\n",
    "\n",
    "    container_width, container_height = image.size\n",
    "    \n",
    "    # loop through all possible extend directions\n",
    "    extend_directions_pair = {\"price_bounds\":[[ExtendDirection.BOTTOM, ExtendDirection.LEFT], [ExtendDirection.BOTTOM, ExtendDirection.RIGHT]],\n",
    "                                \"chinese_bbox\": [[ExtendDirection.BOTTOM, ExtendDirection.LEFT], [ExtendDirection.BOTTOM, ExtendDirection.RIGHT], [ExtendDirection.TOP, ExtendDirection.LEFT], [ExtendDirection.TOP, ExtendDirection.RIGHT]]\n",
    "                              }\n",
    "    \n",
    "    # find the extend direction that have the highest semantic correlation in chinese and english\n",
    "    max_correlation = 0\n",
    "    max_correlation_pair = None\n",
    "    max_locator = None\n",
    "    max_overlap = 0\n",
    "    max_avg_correlation = 0\n",
    "\n",
    "    max_grouped_list, max_grouped_box = None, None  \n",
    "    for overlap_threshold in overlap_threshold_list:\n",
    "        for locator_name, locator_bound in locator_bounds.items():\n",
    "\n",
    "            for extend_directions in extend_directions_pair[locator_name]:\n",
    "                # print()\n",
    "                # print(\"Processing\", extend_directions)\n",
    "                extended_boxes = extend_bounding_boxes(locator_bound, container_width, container_height, extend_directions=extend_directions)\n",
    "                sorted_bounding_boxes = sorted(filtered_bounds, key=lambda bbox: (bbox.y_min, bbox.x_min))\n",
    "                grouped_list, grouped_box = group_extended_boxes(extended_boxes, sorted_bounding_boxes, overlap_threshold=overlap_threshold)\n",
    "                \n",
    "                total_correlation = 0\n",
    "                correlation_count = 0\n",
    "                for string_list in grouped_list:\n",
    "                    # flatten the list of strings by joining them\n",
    "                    string_list = [\" \".join(string) for string in string_list if string != \"\"]\n",
    "                    \n",
    "                    chinese_text, english_text = split_chinese_english(string_list)\n",
    "                    chinese_text = \"\".join(chinese_text)\n",
    "                    english_text = \" \".join(english_text)\n",
    "                    \n",
    "                    correlation = calculate_semantic_correlation(chinese_text, english_text)\n",
    "                    # print(chinese_text, english_text, correlation)\n",
    "                    total_correlation += correlation\n",
    "                    correlation_count += 1\n",
    "                \n",
    "                # Calculate average correlation if there are valid correlations\n",
    "                if correlation_count > 0:\n",
    "                    # avg_correlation = total_correlation / correlation_count\n",
    "                    if total_correlation > max_avg_correlation:\n",
    "                        max_avg_correlation = total_correlation\n",
    "                        max_correlation_pair = extend_directions\n",
    "                        max_grouped_list = grouped_list\n",
    "                        max_grouped_box = grouped_box\n",
    "                        max_locator = locator_bound\n",
    "                        max_overlap = overlap_threshold\n",
    "    \n",
    "    return max_grouped_list, max_grouped_box\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MenuProcessor:\n",
    "    def __init__(self, dir_path):\n",
    "        self.dir_path = dir_path\n",
    "        self.processed_files = []\n",
    "        self.saving_progress = True\n",
    "        self.progress_file_path = 'progress.json'\n",
    "        print(\"MenuProcessor Initialized.\")\n",
    "        self.setup_filepath()\n",
    "\n",
    "    def setup_filepath(self):\n",
    "        print(\"Setting up file paths...\")\n",
    "\n",
    "        try:\n",
    "            with open(self.progress_file_path, 'r') as f:\n",
    "                self.processed_files = json.load(f)\n",
    "            print(\"Progress file loaded.\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"Progress file not found, starting fresh.\")\n",
    "            self.processed_files = []\n",
    "\n",
    "    def process_files(self):\n",
    "        all_files = [f for f in os.listdir(self.dir_path) if os.path.isfile(os.path.join(self.dir_path, f))]\n",
    "        sorted_files = sort_filenames(all_files)\n",
    "\n",
    "        for file_name in tqdm(sorted_files, desc='Processing files'):\n",
    "            if file_name not in self.processed_files:\n",
    "                self.prepare_file_paths(file_name)\n",
    "                self.process_menu_segmentation()\n",
    "                self.save_progress()\n",
    "\n",
    "        print(\"All files have been processed.\")\n",
    "\n",
    "    def prepare_file_paths(self, file_name):\n",
    "        self.file_name = file_name\n",
    "        self.image_path = os.path.join(self.dir_path, self.file_name)\n",
    "        file_name_without_extension = os.path.splitext(self.file_name)[0]\n",
    "\n",
    "        self.raw_ocr_directory = '../../dataset/menu_photo_ocr_raw/'\n",
    "        raw_ocr_filename = file_name_without_extension + \"_raw_annotation.json\"\n",
    "        self.raw_ocr_path = os.path.join(self.raw_ocr_directory, raw_ocr_filename)\n",
    "\n",
    "        self.preprocessed_ocr_directory = '../../dataset/unverified_menu_text/'\n",
    "        preprocessed_ocr_filename = file_name_without_extension + \"_prep_ocr.json\"\n",
    "        self.preprocessed_ocr_path = os.path.join(self.preprocessed_ocr_directory, preprocessed_ocr_filename)\n",
    "        # print(f\"Processing file: {self.file_name}\")\n",
    "\n",
    "    def process_menu_segmentation(self):\n",
    "        if not os.path.exists(self.raw_ocr_path):\n",
    "            print(f\"Warning: OCR file {self.raw_ocr_path} does not exist. Skipping this file.\")\n",
    "            self.processed_files.append(self.file_name)  # Add the file to processed_files to skip it in future\n",
    "            return\n",
    "\n",
    "        max_grouped_list, max_grouped_box = process_menu(self.image_path)\n",
    "\n",
    "        # print(\"Saving segmented menu...\")\n",
    "        \n",
    "        self.save_segmented_menu(max_grouped_list)\n",
    "\n",
    "    def save_segmented_menu(self, grouped_list):\n",
    "        dish_instance_list = []\n",
    "        if grouped_list is None:\n",
    "            return\n",
    "        \n",
    "        for string_list in grouped_list:\n",
    "            dish = segment_dish_text_list(string_list)\n",
    "            dish_instance_list.append(dish)\n",
    "\n",
    "        results = [obj.to_dict() for obj in dish_instance_list]\n",
    "        self.processed_files.append(self.file_name)\n",
    "        save_json(results, self.preprocessed_ocr_path, verbose=False)\n",
    "        # print(f\"Segmented menu saved to {self.preprocessed_ocr_path}\")\n",
    "\n",
    "    def save_progress(self):\n",
    "        if self.saving_progress:\n",
    "            # print(\"Saving progress...\")\n",
    "            with open(self.progress_file_path, 'w') as f:\n",
    "                json.dump(self.processed_files, f)\n",
    "            # print(\"Progress saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MenuProcessor Initialized.\n",
      "Setting up file paths...\n",
      "Progress file loaded.\n",
      "Setting up file paths...\n",
      "Progress file loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 114/114 [05:43<00:00,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dir_path = '../../dataset/menu_photo/segment_by_price'\n",
    "processor = MenuProcessor(dir_path)\n",
    "processor.saving_progress = True\n",
    "    \n",
    "processor.setup_filepath()\n",
    "processor.process_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
