{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import random\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.service import Service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_path = r\"F:\\Fork_git\\Labelling_Menu_Data\\menu_scraper\\webdriver\\chromedriver\\chromedriver.exe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('england_city_codes.json')\n",
    " \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "england_city_code = json.load(f)\n",
    " \n",
    " \n",
    "# Closing file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_categories = {'Japanese', 'Singaporean', 'Indian', 'Korean', 'American', 'Type not available', 'Thai', 'Vietnamese', 'British', 'Italian'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update function to get links and names as per the provided HTML snippet\n",
    "def get_links_and_names(driver):\n",
    "    # Locate the entire restaurant divs\n",
    "    restaurant_divs = driver.find_elements(By.XPATH, '//div[@class=\"yJIls z y M0\"]')\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Iterating through each restaurant div\n",
    "    for restaurant_div in restaurant_divs:\n",
    "        try:\n",
    "            # Finding name and link\n",
    "            name_link_elem = restaurant_div.find_element(By.XPATH, './/a[@href and @target=\"_blank\" and contains(@class, \"BMQDV\")]')\n",
    "            name = name_link_elem.text\n",
    "            link = name_link_elem.get_attribute('href')\n",
    "            \n",
    "            # Finding restaurant type\n",
    "            # Note: Might not be available for all entries, handle accordingly\n",
    "            elems = restaurant_div.find_elements(By.XPATH, './/span[@class=\"YECgr\"]')\n",
    "\n",
    "            restaurant_type = \"Type not available\"\n",
    "            price = \"Price not available\"\n",
    "\n",
    "\n",
    "\n",
    "            # Iterate through the found elements and check for the desired conditions\n",
    "            for elem in elems:\n",
    "                text = elem.text\n",
    "                \n",
    "                # Check if \"Chinese\" is in text, then it's a type\n",
    "                if any(symbol in text.lower() for symbol in ['chinese']):\n",
    "                    restaurant_type = text\n",
    "                # Check if any of the currency symbols are in text, then it's a price\n",
    "                elif any(symbol in text for symbol in [\"£\", \"$\", \"€\"]):  # you can add more currencies here\n",
    "                    price = text\n",
    "\n",
    "            categories = {category.strip() for category in restaurant_type.split(',')}        \n",
    "            if not categories.intersection(excluded_categories):\n",
    "                # Append the restaurant data to the list\n",
    "                data.append({\n",
    "                    'name': name, \n",
    "                    'link': link, \n",
    "                    'type': restaurant_type, \n",
    "                    'price': price\n",
    "                })\n",
    "        except Exception as e:\n",
    "            # Log or print exception info and continue with the next restaurant\n",
    "            print(f\"Skipping a restaurant due to an error: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Update function to get page links as per the provided HTML snippet\n",
    "def get_page_links(driver):\n",
    "    # XPath to locate the next page button based on the provided HTML snippet\n",
    "    next_button = driver.find_elements(By.XPATH, '//a[@data-smoke-attr=\"pagination-next-arrow\"]')\n",
    "    return [btn.get_attribute('href') for btn in next_button]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the existing JSON file at the start\n",
    "filename = 'trip_advisor_manchester.json'\n",
    "if os.path.exists(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        all_shop_data = json.load(f)\n",
    "else:\n",
    "    all_shop_data = []\n",
    "\n",
    "base_page = \"https://www.tripadvisor.co.uk/FindRestaurants?geo=187069&cuisines=5379&establishmentTypes=10591&priceTypes=10954%2C10955&broadened=false\"\n",
    "\n",
    "if all_shop_data and all_shop_data[0]['name'] == 'Stopping page':\n",
    "    # Replace the first element\n",
    "    base_page = all_shop_data[0]['link']\n",
    "\n",
    "\n",
    "\n",
    "service = Service(executable_path=driver_path)\n",
    "\n",
    "options = Options()\n",
    "# options.add_argument(\"--headless\")  # Enable headless mode\n",
    "# options.add_argument(f'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{random.randint(70, 91)}.0.4472.124 Safari/537.36')\n",
    "\n",
    "driver = webdriver.Chrome(service=service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please login to the website... and press Enter here to continue.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.get(base_page)\n",
    "\n",
    "time.sleep(2 + random.random() * 5)  # Random delay between 5 and 10 seconds\n",
    "\n",
    "print(\"Please login to the website... and press Enter here to continue.\")\n",
    "input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_links = get_page_links(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing page 1\n",
      "Accessing page 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "continue_flag = True\n",
    "\n",
    "page_count = 1\n",
    "\n",
    "while continue_flag:\n",
    "    # Process all the links\n",
    "    print(f'Accessing page {page_count}')\n",
    "    time.sleep(5 + random.random() * 5)  # Random delay between 5 and 10 seconds\n",
    "    nav_links = get_page_links(driver)\n",
    "    shop_data = get_links_and_names(driver)\n",
    "\n",
    "    # Update the list with new data, ensuring that no duplicates are added\n",
    "    for shop in shop_data:\n",
    "        if shop not in all_shop_data:\n",
    "            all_shop_data.append(shop)\n",
    "\n",
    "    page_count += 1\n",
    "\n",
    "    if len(nav_links) < 1:\n",
    "        continue_flag = False\n",
    "    else:\n",
    "        driver.get(nav_links[0])\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopping_page = nav_links[0]\n",
    "# # Your new data to insert if condition is met\n",
    "# new_data = {\n",
    "#     'name': 'Stopping page', \n",
    "#     'link': stopping_page, \n",
    "#     'type': '', \n",
    "#     'price': ''\n",
    "# }\n",
    "\n",
    "# # Check if 'all_shop_data' is not empty and if first element has name 'Stopping page'\n",
    "# if all_shop_data and all_shop_data[0]['name'] == 'Stopping page':\n",
    "#     # Replace the first element\n",
    "#     all_shop_data[0] = new_data\n",
    "# else:\n",
    "#     # Otherwise, insert new_data at the beginning of the list\n",
    "#     all_shop_data.insert(0, new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 146 total shop data to trip_advisor_manchester.json.\n"
     ]
    }
   ],
   "source": [
    "# Dump the updated list back into the JSON file\n",
    "with open(filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_shop_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Saved {len(all_shop_data)} total shop data to {filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sleep(min_time=1, max_time=3):\n",
    "    time.sleep(random.uniform(min_time, max_time))\n",
    "\n",
    "# Define a function to download and save images from URLs\n",
    "def download_image(img_url, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        urllib.request.urlretrieve(img_url, save_path)\n",
    "        print(f'Saved image: {save_path}')\n",
    "\n",
    "# Define a function to get image URLs\n",
    "def get_image_urls(driver):\n",
    "    elems = driver.find_elements(By.XPATH, '//div[@class=\"img\"]/a/img')\n",
    "    return [elem.get_attribute('src') for elem in elems]\n",
    "\n",
    "def get_high_res_image_url(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//div[@class=\"main-pic-stage\"]/img')))\n",
    "        elem = driver.find_element(By.XPATH, '//div[@class=\"main-pic-stage\"]/img')\n",
    "        return elem.get_attribute('src')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def go_to_next_image(driver):\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, '//a[@class=\"next J_pic-next\"]')\n",
    "        ActionChains(driver).click(next_button).perform()\n",
    "        return True\n",
    "    except NoSuchElementException:\n",
    "        return False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Translation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
